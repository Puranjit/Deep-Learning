{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e9f5744",
   "metadata": {},
   "source": [
    "# PyTorch \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/pytorch/pytorch/master/docs/source/_static/img/pytorch-logo-dark.png\"\n",
    "        style=\"width:300px; float: right; margin: 0 40px 40px 40px;\"></img>\n",
    "\n",
    "PyTorch is an open source machine learning library based on the Torch library, used for applications such as computer vision and natural language processing, primarily developed by Facebook's AI Research lab.\n",
    "\n",
    "PyTorch is a Python package that provides high-level features:\n",
    "* `Tensor` computations with high GPU accelaration (replacement for NumPy to use the power of GPUs)\n",
    "* Deep neural networks built on a tape-based `autograd` system\n",
    "* A deep learning research platform that provides maximum flexibility and speed.\n",
    "\n",
    "You can reuse your favorite Python packages such as NumPy, SciPy, and Cython to extend PyTorch when needed.\n",
    "\n",
    "You can visit this [link](https://pytorch.org/get-started/locally/) to check the latest stable version for PyTorch that is recommended for your machine.\n",
    "\n",
    "At a granular level, PyTorch is a library that consists of the following components:\n",
    "* `torch` - a Tensor library like NumPy, with strong GPU support\n",
    "* `torch.autograd` - a tape-based automatic differentiation library that supports all differentiable Tensor operations in torch\n",
    "* `torch.jit` - a compilation stack (TorchScript) to create serializable and optimizable models from PyTorch code\n",
    "* `torch.nn` - a neural networks library deeply integrated with autograd designed for maximum flexibility\n",
    "* `torch.multiprocessing` - Python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and Hogwild training\n",
    "* `torch.utils` - DataLoader and other utility functions for convenience \n",
    "\n",
    "### A GPU ready tensor library\n",
    "PyTorch provides Tensors that can live either on the CPU or the GPU and accelerates the computation by a huge amount and provides a wide variety of tensor routines to accelerate and fit your scientific computation needs such as slicing, indexing, math operations, linear algebra, reductions. And they are fast!\n",
    "\n",
    "### Dynamic Neural Networks: Tape-Based Autograd\n",
    "\n",
    "PyTorch has a unique way of building neural networks: using and replaying a tape recorder.\n",
    "\n",
    "Most frameworks such as TensorFlow, Theano, Caffe, and CNTK have a static view of the world. One has to build a neural network and reuse the same structure again and again. Changing the way the network behaves means that one has to start from scratch.\n",
    "\n",
    "PyTorch uses a technique called reverse-mode auto-differentiation, which allows you to change the way your network behaves arbitrarily with zero lag or overhead. Our inspiration comes from several research papers on this topic, as well as current and past work such as torch-autograd, autograd, Chainer, etc.\n",
    "\n",
    "While this technique is not unique to PyTorch, it's one of the fastest implementations of it to date. You get the best of speed and flexibility for your crazy research.\n",
    "\n",
    "### Python First\n",
    "PyTorch is not a Python binding into a monolithic C++ framework. It is built to be deeply integrated into Python. You can use it naturally like you would use NumPy / SciPy / scikit-learn etc. You can write your new neural network layers in Python itself, using your favorite libraries and use packages such as Cython and Numba. Goal is to not reinvent the wheel where appropriate.\n",
    "\n",
    "### Fast and Lean\n",
    "PyTorch has minimal framework overhead. Several acceleration libraries such as Intel MKL and NVIDIA (cuDNN, NCCL) are used to maximize speed. At the core, its CPU and GPU Tensor and neural network backends are mature and have been tested for years.\n",
    "\n",
    "Hence, PyTorch is quite fast â€“ whether you run small or large neural networks.\n",
    "\n",
    "The memory usage in PyTorch is extremely efficient compared to Torch or some of the alternatives. We've written custom memory allocators for the GPU to make sure that your deep learning models are maximally memory efficient. This enables user to train bigger deep learning models than before.\n",
    "\n",
    "### Extensions Without Pain\n",
    "Writing new neural network modules, or interfacing with PyTorch's Tensor API was designed to be straightforward and with minimal abstractions.\n",
    "\n",
    "You can write new neural network layers in Python using the torch API or your favorite NumPy-based libraries such as SciPy.\n",
    "\n",
    "\n",
    "## Background\n",
    "Neural networks (NNs) are a collection of nested functions that are executed on some input data. These functions are defined by parameters (consisting of weights and biases), which in PyTorch are stored in tensors.\n",
    "\n",
    "Training a NN happens in two steps:\n",
    "\n",
    "* `Forward Propagation`: In forward prop, the NN makes its best guess about the correct output. It runs the input data through each of its functions to make this guess.\n",
    "\n",
    "* `Backward Propagation`: In backprop, the NN adjusts its parameters proportionate to the error in its guess. It does this by traversing backwards from the output, collecting the derivatives of the error with respect to the parameters of the functions (gradients), and optimizing the parameters using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeb0795",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
